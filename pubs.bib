@article{TSAI201947,
title = {Under-sampling class imbalanced datasets by combining clustering analysis and instance selection},
journal = {Information Sciences},
volume = {477},
pages = {47-54},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.10.029},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518308478},
author = {Chih-Fong Tsai and Wei-Chao Lin and Ya-Han Hu and Guan-Ting Yao},
keywords = {Data mining, Class imbalance, Clustering, Ensemble classifiers, Instance selection},
abstract = {Class-imbalanced datasets, i.e., those with the number of data samples in one class being much larger than that in another class, occur in many real-world problems. Using these datasets, it is very difficult to construct effective classifiers based on the current classification algorithms, especially for distinguishing small or minority classes from the majority class. To solve the class imbalance problem, the under/oversampling techniques have been widely used to reduce and enlarge the numbers of data samples in the majority and minority classes, respectively. Moreover, the combinations of certain sampling approaches with ensemble classifiers have shown reasonably good performance. In this paper, a novel undersampling approach called cluster-based instance selection (CBIS) that combines clustering analysis and instance selection is introduced. The clustering analysis component groups similar data samples of the majority class dataset into ‘subclasses’, while the instance selection component filters out unrepresentative data samples from each of the ‘subclasses’. The experimental results based on the KEEL dataset repository show that the CBIS approach can make bagging and boosting-based MLP ensemble classifiers perform significantly better than six state-of-the-art approaches, regardless of what kinds of clustering (affinity propagation and k-means) and instance selection (IB3, DROP3 and GA) algorithms are used.}
}

@INPROCEEDINGS{7816918,
  author={Taleb, Ikbal and Kassabi, Hadeel T. El and Serhani, Mohamed Adel and Dssouli, Rachida and Bouhaddioui, Chafik},
  booktitle={2016 Intl IEEE Conferences on Ubiquitous Intelligence & Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)}, 
  title={Big Data Quality: A Quality Dimensions Evaluation}, 
  year={2016},
  volume={},
  number={},
  pages={759-765},
  doi={10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0122}}

@article{lapedriza2013all,
  title={Are all training examples equally valuable?},
  author={Lapedriza, Agata and Pirsiavash, Hamed and Bylinskii, Zoya and Torralba, Antonio},
  journal={arXiv preprint arXiv:1311.6510},
  year={2013}
}

@inproceedings{mall2014representative,
  title={Representative subsets for big data learning using k-NN graphs},
  author={Mall, Raghvendra and Jumutc, Vilen and Langone, Rocco and Suykens, Johan AK},
  booktitle={2014 IEEE International Conference on Big Data (Big Data)},
  pages={37--42},
  year={2014},
  organization={IEEE}
}

@book{karahoca2012advances,
  title={Advances in data mining knowledge discovery and applications},
  author={Karahoca, Adem},
  year={2012},
  publisher={BoD--Books on Demand}
}

@article{tang2019very,
  title={Very large-scale data classification based on K-means clustering and multi-kernel SVM},
  author={Tang, Tinglong and Chen, Shengyong and Zhao, Meng and Huang, Wei and Luo, Jake},
  journal={Soft Computing},
  volume={23},
  number={11},
  pages={3793--3801},
  year={2019},
  publisher={Springer}
}

@article{DASZYKOWSKI200291,
title = {Representative subset selection},
journal = {Analytica Chimica Acta},
volume = {468},
number = {1},
pages = {91-103},
year = {2002},
issn = {0003-2670},
doi = {https://doi.org/10.1016/S0003-2670(02)00651-7},
url = {https://www.sciencedirect.com/science/article/pii/S0003267002006517},
author = {M. Daszykowski and B. Walczak and D.L. Massart},
keywords = {Data mining, Subset selection, Uniform design},
abstract = {Fast development of analytical techniques enable to acquire huge amount of data. Large data sets are difficult to handle and therefore, there is a big interest in designing a subset of the original data set, which preserves the information of the original data set and facilitates the computations. There are many subset selection methods and their choice depends on the problem at hand. The two most popular groups of subset selection methods are uniform designs and cluster-based designs. Among the methods considered in this paper there are uniform designs, such as those proposed by Kennard and Stone, OptiSim, and cluster-based designs applying K-means technique and density based spatial clustering of applications with noise (DBSCAN). Additionally, a new concept of the subset selection with K-means is introduced.}
}

@InProceedings{10.1007/978-3-540-69052-8_29,
author="Czarnowski, Ireneusz
and Jȩdrzejowicz, Piotr",
editor="Nguyen, Ngoc Thanh
and Borzemski, Leszek
and Grzech, Adam
and Ali, Moonis",
title="Data Reduction Algorithm for Machine Learning and Data Mining",
booktitle="New Frontiers in Applied Artificial Intelligence",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="276--285",
abstract="The paper proposes an approach to data reduction. The data reduction procedures are of vital importance to machine learning and data mining. To solve the data reduction problems the agent-based population learning algorithm was used. The proposed approach has been used to reduce the original dataset in two dimensions including selection of reference instances and removal of irrelevant attributes. To validate the approach the computational experiment has been carried out. Presentation and discussion of experiment results conclude the paper.",
isbn="978-3-540-69052-8"
}

@InProceedings{10.1007/978-3-540-87481-2_10,
author="Nguyen, XuanLong
and Huang, Ling
and Joseph, Anthony D.",
editor="Daelemans, Walter
and Goethals, Bart
and Morik, Katharina",
title="Support Vector Machines, Data Reduction, and Approximate Kernel Matrices",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="137--153",
abstract="The computational and/or communication constraints associated with processing large-scale data sets using support vector machines (SVM) in contexts such as distributed networking systems are often prohibitively high, resulting in practitioners of SVM learning algorithms having to apply the algorithm on approximate versions of the kernel matrix induced by a certain degree of data reduction. In this paper, we study the tradeoffs between data reduction and the loss in an algorithm's classification performance. We introduce and analyze a consistent estimator of the SVM's achieved classification error, and then derive approximate upper bounds on the perturbation on our estimator. The bound is shown to be empirically tight in a wide range of domains, making it practical for the practitioner to determine the amount of data reduction given a permissible loss in the classification performance.",
isbn="978-3-540-87481-2"
}

@article{OUGIAROGLOU2018101,
title = {Exploring the effect of data reduction on Neural Network and Support Vector Machine classification},
journal = {Neurocomputing},
volume = {280},
pages = {101-110},
year = {2018},
note = {Applications of Neural Modeling in the new era for data and IT},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.08.076},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217317757},
author = {Stefanos Ougiaroglou and Konstantinos I. Diamantaras and Georgios Evangelidis},
keywords = {Neural Networks, Support Vector Machines, -NN classification, Data reduction, Prototype selection, Prototype generation, Condensing},
abstract = {Neural Networks and Support Vector Machines (SVMs) are two of the most popular and efficient supervised classification models. However, in the context of large datasets many complexity issues arise due to high memory requirements and high computational cost. In the context of the application of Data Mining algorithms, data reduction techniques attempt to reduce the size of training datasets in terms of the number of instances by selecting some of the existing instances or by generating new training instances. The idea is to speed up the application of the data mining algorithm with minimum or no sacrifice in performance. Data reduction techniques have been extensively used in the context of k-Nearest Neighbor classification, a lazy classifier that works by directly using a training dataset rather than building a model. This paper explores the application of data reduction techniques as a preprocessing step before the training step of Neural Networks and SVMs. Furthermore, the paper proposes a new data reduction technique that is based on k-median clustering algorithm. Our experimental results illustrate that, in the case of SVMs, data reduction techniques can effectively reduce the dataset size incurring small performance degradation. In the case of Neural Networks, the performance loss is somewhat greater, for the same data reduction rate, but both SVM and Neural Network models outperform the k-NN approach that is typically used in Data Mining applications.}
}

@article{ARORA2016507,
title = {Analysis of K-Means and K-Medoids Algorithm For Big Data},
journal = {Procedia Computer Science},
volume = {78},
pages = {507-512},
year = {2016},
note = {1st International Conference on Information Security & Privacy 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.02.095},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916000971},
author = {Preeti Arora and  Deepali and Shipra Varshney},
keywords = {Clustering, K-Means, K-Medoids},
abstract = {Clustering plays a very vital role in exploring data, creating predictions and to overcome the anomalies in the data. Clusters that contain collateral, identical characteristics in a dataset are grouped using reiterative techniques. As the data in real world is growing day by day so very large datasets with little or no background knowledge can be identified into interesting patterns with clustering. So, in this paper the two most popular clustering algorithms K-Means and K-Medoids are evaluated on dataset transaction10k of KEEL. The input to these algorithms are randomly distributed data points and based on their similarity clusters has been generated. The comparison results show that time taken in cluster head selection and space complexity of overlapping of cluster is much better in K-Medoids than K-Means. Also K-Medoids is better in terms of execution time, non sensitive to outliers and reduces noise as compared to K-Means as it minimizes the sum of dissimilarities of data objects.}
}

@ARTICLE{7906512,
  author={L’Heureux, Alexandra and Grolinger, Katarina and Elyamany, Hany F. and Capretz, Miriam A. M.},
  journal={IEEE Access}, 
  title={Machine Learning With Big Data: Challenges and Approaches}, 
  year={2017},
  volume={5},
  number={},
  pages={7776-7797},
  doi={10.1109/ACCESS.2017.2696365}}
  
  
@InProceedings{pmlr-v139-s21a,
  title = 	 {Training Data Subset Selection for Regression with Controlled Generalization Error},
  author =       {S, Durga and Iyer, Rishabh and Ramakrishnan, Ganesh and De, Abir},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9202--9212},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/s21a/s21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/s21a.html},
  abstract = 	 {Data subset selection from a large number of training instances has been a successful approach toward efficient and cost-effective machine learning. However, models trained on a smaller subset may show poor generalization ability. In this paper, our goal is to design an algorithm for selecting a subset of the training data, so that the model can be trained quickly, without significantly sacrificing on accuracy. More specifically, we focus on data subset selection for $L_2$ regularized regression problems and provide a novel problem formulation which seeks to minimize the training loss with respect to both the trainable parameters and the subset of training data, subject to error bounds on the validation set. We tackle this problem using several technical innovations. First, we represent this problem with simplified constraints using the dual of the original training problem and show that the objective of this new representation is a monotone and $\alpha$-submodular function, for a wide variety of modeling choices. Such properties lead us to develop SELCON, an efficient majorization-minimization algorithm for data subset selection, that admits an approximation guarantee even when the training provides an imperfect estimate of the trained model. Finally, our experiments on several datasets show that SELCON trades off accuracy and efficiency more effectively than the current state-of-the-art.}
}

@article{CAI201870,
title = {Feature selection in machine learning: A new perspective},
journal = {Neurocomputing},
volume = {300},
pages = {70-79},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.11.077},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218302911},
author = {Jie Cai and Jiawei Luo and Shulin Wang and Sheng Yang},
keywords = {Feature selection, Dimensionality reduction, Machine learning, Data mining},
abstract = {High-dimensional data analysis is a challenge for researchers and engineers in the fields of machine learning and data mining. Feature selection provides an effective way to solve this problem by removing irrelevant and redundant data, which can reduce computation time, improve learning accuracy, and facilitate a better understanding for the learning model or data. In this study, we discuss several frequently-used evaluation measures for feature selection, and then survey supervised, unsupervised, and semi-supervised feature selection methods, which are widely applied in machine learning problems, such as classification and clustering. Lastly, future challenges about feature selection are discussed.}
}

@INPROCEEDINGS{8658965,
  author={Kaushal, Vishal and Iyer, Rishabh and Kothawade, Suraj and Mahadev, Rohan and Doctor, Khoshrav and Ramakrishnan, Ganesh},
  booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Learning From Less Data: A Unified Data Subset Selection and Active Learning Framework for Computer Vision}, 
  year={2019},
  volume={},
  number={},
  pages={1289-1299},
  doi={10.1109/WACV.2019.00142}}
  
  @ARTICLE{7364258,
  author={Elhamifar, Ehsan and Sapiro, Guillermo and Sastry, S. Shankar},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Dissimilarity-Based Sparse Subset Selection}, 
  year={2016},
  volume={38},
  number={11},
  pages={2182-2197},
  doi={10.1109/TPAMI.2015.2511748}}

@article{koggalage2004reducing,
  title={Reducing the number of training samples for fast support vector machine classification},
  author={Koggalage, Ravindra and Halgamuge, Saman},
  journal={Neural Information Processing-Letters and Reviews},
  volume={2},
  number={3},
  pages={57--65},
  year={2004}
}

@article{FOODY20061,
title = {Training set size requirements for the classification of a specific class},
journal = {Remote Sensing of Environment},
volume = {104},
number = {1},
pages = {1-14},
year = {2006},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2006.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0034425706001234},
author = {Giles M. Foody and Ajay Mathur and Carolina Sanchez-Hernandez and Doreen S Boyd},
keywords = {Classsification, Training set, Support vector machine (SVM), Support vector data description (SVDD)},
abstract = {The design of the training stage of a supervised classification should account for the properties of the classifier to be used. Consideration of the way the classifier operates may enable the training stage to be designed in a manner which ensures that the aim of the classification is satisfied with the use of a small, inexpensive, training set. It may, therefore, be possible to reduce the training set size requirements from that generally expected with the use of standard heuristics. Substantial reductions in training set size may be possible if interest is focused on a single class. This is illustrated for mapping cotton in north-western India by support vector machine type classifiers. Four approaches to reducing training set size were used: intelligent selection of the most informative training samples, selective class exclusion, acceptance of imprecise descriptions for spectrally distinct classes and the adoption of a one-class classifier. All four approaches were able to reduce the training set size required considerably below that suggested by conventional widely used heuristics without significant impact on the accuracy with which the class of interest was classified. For example, reductions in training set size of ∼90% from that suggested by a conventional heuristic are reported with the accuracy of cotton classification remaining nearly constant at ∼95% and ∼97% from the user's and producer's perspectives respectively.}
}

@INPROCEEDINGS{4761793,
  author={Xiao-Zhang Liu and Guo-Can Feng},
  booktitle={2008 19th International Conference on Pattern Recognition}, 
  title={Kernel Bisecting k-means clustering for SVM training sample reduction}, 
  year={2008},
  volume={},
  number={},
  pages={1-4},
  doi={10.1109/ICPR.2008.4761793}}


@article{BLUM1997245,
title = {Selection of relevant features and examples in machine learning},
journal = {Artificial Intelligence},
volume = {97},
number = {1},
pages = {245-271},
year = {1997},
note = {Relevance},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00063-5},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000635},
author = {Avrim L. Blum and Pat Langley},
keywords = {Relevant features, Relevant examples, Machine learning},
abstract = {In this survey, we review work in machine learning on methods for handling data sets containing large amounts of irrelevant information. We focus on two key issues: the problem of selecting relevant features, and the problem of selecting relevant examples. We describe the advances that have been made on these topics in both empirical and theoretical work in machine learning, and we present a general framework that we use to compare different methods. We close with some challenges for future work in this area.}
}

@ARTICLE{1255391,
  author={Cano, J.R. and Herrera, F. and Lozano, M.},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Using evolutionary algorithms as instance selection for data reduction in KDD: an experimental study}, 
  year={2003},
  volume={7},
  number={6},
  pages={561-575},
  doi={10.1109/TEVC.2003.819265}}

@article{CAVALCANTI20136894,
title = {ATISA: Adaptive Threshold-based Instance Selection Algorithm},
journal = {Expert Systems with Applications},
volume = {40},
number = {17},
pages = {6894-6900},
year = {2013},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2013.06.053},
url = {https://www.sciencedirect.com/science/article/pii/S095741741300448X},
author = {George D.C. Cavalcanti and Tsang Ing Ren and Cesar Lima Pereira},
keywords = {Instance selection, Instance-based learning algorithms},
abstract = {Instance reduction techniques can improve generalization, reduce storage requirements and execution time of instance-based learning algorithms. This paper presents an instance reduction algorithm called Adaptive Threshold-based Instance Selection Algorithm (ATISA). ATISA aims to preserve important instances based on a selection criterion that uses the distance of each instance to its nearest enemy as a threshold. This threshold defines the coverage area of each instance that is given by a hyper-sphere centered at it. The experimental results show the effectiveness, in terms of accuracy, reduction rate, and computational time, of the ATISA algorithm when compared with state-of-the-art reduction algorithms.}
}
